{
 "cells": [
  {
   "source": [
    "# How To mesure beauty\n",
    "\n",
    "## 1. Nombre d'or\n",
    "> ### - La largeur de la bouche est 1.618 fois la largeur du nez\n",
    "\n",
    "> ### - La largeur de la bouche est 1.618 fois l'intervalle qui la separe de l'extremité du visage\n",
    "\n",
    "> ### - Le nez etant un triangle, les cotés font 1.618 fois la base\n",
    "\n",
    "## 2. Facial Keypoint Detection (Dataset)\n",
    "> ### - Le dataset contient 15 FKPs (paper)\n",
    "\n",
    "> ### - Le dataset contient 68 FKPs (an other dataset)\n",
    "\n",
    "> ### - Chaque FKP a deux coordonnées spatiales (x, y)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Targered FKPs\n",
    "\n",
    "## MOUTH    : 49 - 55\n",
    "## NOSE\n",
    "> ## PEEK   : 28\n",
    "> ## BASE   : 32 - 36\n",
    "\n",
    "## EDGES\n",
    "> ## LEFT   : 4\n",
    "> ## RIGHT  : 14"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# WORKFLOW\n",
    "\n",
    "## 1. Data Preprocessing\n",
    "> ### - Download FKPs dataset\n",
    "> ### - Load and visualize data\n",
    "> ### - Data Transformation (Normalization, Rotation, Rescal, RandomCrop)\n",
    "> ### - Data Iteration and Batching (training & testing)\n",
    "\n",
    "> ### ```python [[IMAGE ARRAY], [X, Y]]```\n",
    "\n",
    "> ### 2 Possibilites\n",
    "> 1. ### Entrainer 68 CNN qui predisent les coordonnees (x, y) d'un FKP \n",
    "- ```python [[IMAGE ARRAY], [X, Y]]```\n",
    "> 2. ### Entrainer 1 CNN qui predit les 136 coordonnees\n",
    "- ```python [[IMAGE ARRAY], [X..., Y...]]```\n",
    "\n",
    "\n",
    "## 2. Defining and Training a Naimish CNN to predict FKPs\n",
    "> ### - Define model with Keras\n",
    "> ### - Define Training Parameters (Epoch, Loss Function, Optimizer)\n",
    "> ### - Train model\n",
    "> ### - Test model\n",
    "\n",
    "## 3. Defining a Face Beauty classifier\n",
    "> ### 1 Mean Square\n",
    "- #### Calcul de la moyenne quadratique\n",
    "\n",
    "> #### 3.1 La largeur de la bouche est 1.618 fois la largeur du nez\n",
    "- #### mouthWidth = distance(FKP49, FKP55)\n",
    "- #### noseWIdth = distance(FKP32, FKP36)\n",
    "- #### ratioMouthNose = mouthWidth / noseWIdth\n",
    "- #### accuracy (taux d'exactitude) = ratioMouthNose / 1.618 = 1.\n",
    "\n",
    "> #### 3.2 La largeur de la bouche est 1.618 fois l'intervalle qui la separe de l'extremité du visage\n",
    "- #### mouthWidth = distance(FKP49, FKP55),\n",
    "- #### leftMouthEdgeInterval = distance(FKP4, FKP49)\n",
    "- #### rightMouthEdgeInterval = distance(FKP55, FKP14)\n",
    "- ### Left Mouth\n",
    "- #### ratioLeftMouthEdgeInterval = mouthWidth / leftMouthEdgeInterval\n",
    "- #### leftAccuracy (taux d'exactitude) = ratioLeftMouthEdgeInterval / 1.618 = 1.\n",
    "- ### Right Mouth\n",
    "- #### ratioRightMouthEdgeInterval = mouthWidth / rightMouthEdgeInterval\n",
    "- #### rightAccuracy (taux d'exactitude) = ratioRightMouthEdgeInterval / 1.618 = 1.\n",
    "\n",
    "> #### 3.3 Le nez etant un triangle, les cotés font 1.618 fois la base\n",
    "- #### noseWIdth = distance(FKP32, FKP36)\n",
    "- #### noseLeft = distance(FKP28, FKP32)\n",
    "- #### noseLeft = distance(FKP28, FKP36)\n",
    "- #### Left Nose\n",
    "- #### ratioLeftBaseNoise = noseLeft / noseWIdth\n",
    "- #### leftNBaseNoiseAccuracy (taux d'exactitude) = ratioLeftBaseNoise / 1.618 = 1.\n",
    "- ### Right Nose\n",
    "- #### ratioRightBaseNoise = noseRight / noseWIdth\n",
    "- #### rightNBaseNoiseAccuracy (taux d'exactitude) = ratioRightBaseNoise / 1.618 = 1.\n",
    "\n",
    "> #### FacialBeautyAccuracy = Sum(RuleAccuracies) / rulesNomber \n",
    "\n",
    "> ### 2. Train an Unsupervised classifier\n",
    "- #### KNN (3 classes)\n",
    "- Note : cela peut etre utilisé comme un algorithme d'etiquetage\n",
    "\n",
    "> ### 3. Train a Supervised Classifier\n",
    "- #### KNN (3 classes) as labeling algorithm\n",
    "- #### Full Connected Neural Network as classifier\n",
    "\n",
    "## 4. Deploying model as an API\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Keypoint Detection\n",
    "  \n",
    "This project will be all about defining and training a convolutional neural network to perform facial keypoint detection, and using computer vision techniques to transform images of faces.  The first step in any challenge like this will be to load and visualize the data you'll be working with. \n",
    "\n",
    "Let's take a look at some examples of images and corresponding facial keypoints.\n",
    "\n",
    "<img src='images/key_pts_example.png' width=100% height=100%/>\n",
    "\n",
    "Facial keypoints (also called facial landmarks) are the small magenta dots shown on each of the faces in the image above. In each training and test image, there is a single face and **68 keypoints, with coordinates (x, y), for that face**.  These keypoints mark important areas of the face: the eyes, corners of the mouth, the nose, etc. These keypoints are relevant for a variety of tasks, such as face filters, emotion recognition, pose recognition, and so on. Here they are, numbered, and you can see that specific ranges of points match different portions of the face.\n",
    "\n",
    "<img src='images/landmarks_numbered.jpg' width=60% height=60%/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Visualize Data\n",
    "\n",
    "The first step in working with any dataset is to become familiar with your data; you'll need to load in the images of faces and their keypoints and visualize them! This set of image data has been extracted from the [YouTube Faces Dataset](https://www.cs.tau.ac.il/~wolf/ytfaces/), which includes videos of people in YouTube videos. These videos have been fed through some processing steps and turned into sets of image frames containing one face and the associated keypoints.\n",
    "\n",
    "#### Training and Testing Data\n",
    "\n",
    "This facial keypoints dataset consists of 5770 color images. All of these images are separated into either a training or a test set of data.\n",
    "\n",
    "* 3462 of these images are training images, for you to use as you create a model to predict keypoints.\n",
    "* 2308 are test images, which will be used to test the accuracy of your model.\n",
    "\n",
    "The information about the images and keypoints in this dataset are summarized in CSV files, which we can read in using `pandas`. Let's read the training CSV and get the annotations in an (N, 2) array where N is the number of keypoints and 2 is the dimension of the keypoint coordinates (x, y).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_pts_frame = pd.read_csv('data/training_frames_keypoints.csv')\n",
    "\n",
    "key_pts_frame.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 0\n",
    "image_name = key_pts_frame.iloc[n, 0]\n",
    "key_pts = key_pts_frame.iloc[n, 1:]#.as_matrix()\n",
    "print(key_pts)\n",
    "key_pts = key_pts.values\n",
    "print(type(key_pts))\n",
    "key_pts = key_pts.astype('float').reshape(-1, 2)\n",
    "\n",
    "print('Image name: ', image_name)\n",
    "print('Landmarks shape: ', key_pts.shape)\n",
    "print('First 4 key pts:\\n{}'.format(key_pts[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print out some stats about the data\n",
    "print('Number of images: ', key_pts_frame.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at some images\n",
    "\n",
    "Below, is a function `show_keypoints` that takes in an image and keypoints and displays them.  As you look at this data, **note that these images are not all of the same size**, and neither are the faces! To eventually train a neural network on these images, we'll need to standardize their shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_keypoints(image, key_pts):\n",
    "    \"\"\"Show image with keypoints\"\"\"\n",
    "    plt.imshow(image)\n",
    "    plt.scatter(key_pts[:, 0], key_pts[:, 1], s=20, marker='.', c='m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display a few different types of images by changing the index n\n",
    "\n",
    "# select an image by index in our data frame\n",
    "n = 0\n",
    "image_name = key_pts_frame.iloc[n, 0]\n",
    "key_pts = key_pts_frame.iloc[n, 1:].values#.as_matrix()\n",
    "key_pts = key_pts.astype('float').reshape(-1, 2)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "show_keypoints(mpimg.imread(os.path.join('data/training/', image_name)), key_pts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class and Transformations\n",
    "\n",
    "To prepare our data for training, we'll be using PyTorch's Dataset class. Much of this this code is a modified version of what can be found in the [PyTorch data loading tutorial](http://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "\n",
    "#### Dataset class\n",
    "\n",
    "``torch.utils.data.Dataset`` is an abstract class representing a\n",
    "dataset. This class will allow us to load batches of image/keypoint data, and uniformly apply transformations to our data, such as rescaling and normalizing images for training a neural network.\n",
    "\n",
    "\n",
    "Your custom dataset should inherit ``Dataset`` and override the following\n",
    "methods:\n",
    "\n",
    "-  ``__len__`` so that ``len(dataset)`` returns the size of the dataset.\n",
    "-  ``__getitem__`` to support the indexing such that ``dataset[i]`` can\n",
    "   be used to get the i-th sample of image/keypoint data.\n",
    "\n",
    "Let's create a dataset class for our face keypoints dataset. We will\n",
    "read the CSV file in ``__init__`` but leave the reading of images to\n",
    "``__getitem__``. This is memory efficient because all the images are not\n",
    "stored in the memory at once but read as required.\n",
    "\n",
    "A sample of our dataset will be a dictionary\n",
    "``{'image': image, 'keypoints': key_pts}``. Our dataset will take an\n",
    "optional argument ``transform`` so that any required processing can be\n",
    "applied on the sample. We will see the usefulness of ``transform`` in the\n",
    "next section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class FacialKeypointsDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.key_pts_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.key_pts_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = os.path.join(self.root_dir,\n",
    "                                self.key_pts_frame.iloc[idx, 0])\n",
    "        \n",
    "        # image matrix\n",
    "        image = mpimg.imread(image_name)\n",
    "        \n",
    "        # if image has an alpha color channel, get rid of it\n",
    "        if(image.shape[2] == 4):\n",
    "            image = image[:,:,0:3]\n",
    "        \n",
    "        key_pts = self.key_pts_frame.iloc[idx, 1:].values#.as_matrix()\n",
    "\n",
    "        key_pts = key_pts.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'keypoints': key_pts}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined this class, let's instantiate the dataset and display some images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct the dataset\n",
    "face_dataset = FacialKeypointsDataset(csv_file='data/training_frames_keypoints.csv',\n",
    "                                      root_dir='data/training/')\n",
    "\n",
    "# print some stats about the dataset\n",
    "print('Length of dataset: ', len(face_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display a few of the images from the dataset\n",
    "num_to_display = 3\n",
    "\n",
    "for i in range(num_to_display):\n",
    "    \n",
    "    # define the size of images\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    \n",
    "    # randomly select a sample\n",
    "    rand_i = np.random.randint(0, len(face_dataset))\n",
    "    sample = face_dataset[rand_i]\n",
    "\n",
    "    # print the shape of the image and keypoints\n",
    "    print(i, sample['image'].shape, sample['keypoints'].shape)\n",
    "\n",
    "    ax = plt.subplot(1, num_to_display, i + 1)\n",
    "    ax.set_title('Sample #{}'.format(i))\n",
    "    \n",
    "    # Using the same display function, defined earlier\n",
    "    show_keypoints(sample['image'], sample['keypoints'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms\n",
    "\n",
    "Now, the images above are not of the same size, and neural networks often expect images that are standardized; a fixed size, with a normalized range for color ranges and coordinates, and (for PyTorch) converted from numpy lists and arrays to Tensors.\n",
    "\n",
    "Therefore, we will need to write some pre-processing code.\n",
    "Let's create four transforms:\n",
    "\n",
    "-  ``Normalize``: to convert a color image to grayscale values with a range of [0,1] and normalize the keypoints to be in a range of about [-1, 1]\n",
    "-  ``Rescale``: to rescale an image to a desired size.\n",
    "-  ``RandomCrop``: to crop an image randomly.\n",
    "-  ``ToTensor``: to convert numpy images to torch images.\n",
    "\n",
    "\n",
    "We will write them as callable classes instead of simple functions so\n",
    "that parameters of the transform need not be passed everytime it's\n",
    "called. For this, we just need to implement ``__call__`` method and \n",
    "(if we require parameters to be passed in), the ``__init__`` method. \n",
    "We can then use a transform like this:\n",
    "\n",
    "    tx = Transform(params)\n",
    "    transformed_sample = tx(sample)\n",
    "\n",
    "Observe below how these transforms are generally applied to both the image and its keypoints.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tranforms\n",
    "\n",
    "class Normalize(object):\n",
    "    \"\"\"Convert a color image to grayscale and normalize the color range to [0,1].\"\"\"        \n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, key_pts = sample['image'], sample['keypoints']\n",
    "        \n",
    "        image_copy = np.copy(image)\n",
    "        key_pts_copy = np.copy(key_pts)\n",
    "\n",
    "        # convert image to grayscale\n",
    "        image_copy = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # scale color range from [0, 255] to [0, 1]\n",
    "        image_copy=  image_copy/255.0\n",
    "        \n",
    "        # scale keypoints to be centered around 0 with a range of [-1, 1]\n",
    "        # mean = 100, sqrt = 50, so, pts should be (pts - 100)/50\n",
    "        key_pts_copy = (key_pts_copy - 100)/50.0\n",
    "\n",
    "\n",
    "        return {'image': image_copy, 'keypoints': key_pts_copy}\n",
    "\n",
    "\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        # output_size est de type int ou tuple\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, key_pts = sample['image'], sample['keypoints']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size # (h, w)\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = cv2.resize(image, (new_w, new_h))\n",
    "        \n",
    "        # scale the pts, too\n",
    "        key_pts = key_pts * [new_w / w, new_h / h]\n",
    "\n",
    "        return {'image': img, 'keypoints': key_pts}\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, key_pts = sample['image'], sample['keypoints']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "        key_pts = key_pts - [left, top]\n",
    "\n",
    "        return {'image': image, 'keypoints': key_pts}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, key_pts = sample['image'], sample['keypoints']\n",
    "         \n",
    "        # if image has no grayscale color channel, add one\n",
    "        if(len(image.shape) == 2):\n",
    "            # add that third color dim\n",
    "            image = image.reshape(image.shape[0], image.shape[1], 1)\n",
    "            \n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        \n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'keypoints': torch.from_numpy(key_pts)}\n",
    "\n",
    "class AddBatchChannel(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, key_pts = sample['image'], sample['keypoints']\n",
    "         \n",
    "        # if image has no grayscale color channel, add one\n",
    "        if(len(image.shape) == 2):\n",
    "            # add that third color dim\n",
    "            image = image.reshape(image.shape[0], image.shape[1], 1)\n",
    "            \n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # batch image: C X H X W\n",
    "        # image = image.transpose((2, 0, 1))\n",
    "        \n",
    "        return {'image': image,\n",
    "                'keypoints': key_pts}"
   ]
  },
  {
   "source": [
    "## Transform Compose"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformCompose(object):\n",
    "    def __init__(self, transforms=[]):\n",
    "        # s'assurer qu'on a la liste de transformation\n",
    "        assert isinstance(transforms, list)\n",
    "\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "            Applique la combinaison de transformations\n",
    "\n",
    "            :return: tuple (np.array, (x, y))\n",
    "        \"\"\"\n",
    "        # nouvelle transformation\n",
    "        newSample = sample\n",
    "\n",
    "        # appliquer les transformations\n",
    "        for transform in self.transforms:\n",
    "            newSample = transform(newSample)\n",
    "\n",
    "        # add chanel\n",
    "        image = newSample['image']\n",
    "         \n",
    "        # if image has no grayscale color channel, add one\n",
    "        if(len(image.shape) == 2):\n",
    "            # add that third color dim\n",
    "            image = image.reshape(image.shape[0], image.shape[1], 1)\n",
    "        \n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "\n",
    "        newSample['image'] = image\n",
    "        \n",
    "        return newSample['image'], newSample['keypoints']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out the transforms\n",
    "\n",
    "Let's test these transforms out to make sure they behave as expected. As you look at each transform, note that, in this case, **order does matter**. For example, you cannot crop a image using a value smaller than the original image (and the orginal images vary in size!), but, if you first rescale the original image, you can then crop it to any size smaller than the rescaled size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test out some of these transforms\n",
    "rescale = Rescale(100)\n",
    "crop = RandomCrop(50)\n",
    "composed = transforms.Compose([Rescale(250),\n",
    "                               RandomCrop(224)])\n",
    "\n",
    "# apply the transforms to a sample image\n",
    "test_num = 500\n",
    "sample = face_dataset[test_num]\n",
    "\n",
    "fig = plt.figure()\n",
    "for i, tx in enumerate([rescale, crop, composed]):\n",
    "    transformed_sample = tx(sample)\n",
    "\n",
    "    ax = plt.subplot(1, 3, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title(type(tx).__name__)\n",
    "    show_keypoints(transformed_sample['image'], transformed_sample['keypoints'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the transformed dataset\n",
    "\n",
    "Apply the transforms in order to get grayscale images of the same shape. Verify that your transform works by printing out the shape of the resulting data (printing out a few examples should show you a consistent tensor size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the data tranform\n",
    "# order matters! i.e. rescaling should come before a smaller crop\n",
    "data_transform = transforms.Compose([Rescale(250),\n",
    "                                     RandomCrop(224),\n",
    "                                     Normalize(),\n",
    "                                     ToTensor()])\n",
    "\n",
    "# create the transformed dataset\n",
    "transformed_dataset = FacialKeypointsDataset(csv_file='data/training_frames_keypoints.csv',\n",
    "                                             root_dir='data/training/',\n",
    "                                             transform=data_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print some stats about the transformed data\n",
    "print('Number of images: ', len(transformed_dataset))\n",
    "\n",
    "# make sure the sample tensors are the expected size\n",
    "for i in range(5):\n",
    "    sample = transformed_dataset[i]\n",
    "    print(i, sample['image'].size(), sample['keypoints'].size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Iteration and Batching\n",
    "\n",
    "Right now, we are iterating over this data using a ``for`` loop, but we are missing out on a lot of PyTorch's dataset capabilities, specifically the abilities to:\n",
    "\n",
    "-  Batch the data\n",
    "-  Shuffle the data\n",
    "-  Load the data in parallel using ``multiprocessing`` workers.\n",
    "\n",
    "``torch.utils.data.DataLoader`` is an iterator which provides all these\n",
    "features, and we'll see this in use in the *next* notebook, Notebook 2, when we load data in batches to train a neural network!\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100  # Number of training examples to process before updating our models variables\n",
    "IMG_SHAPE  = 96  # Our training data consists of images with width of 150 pixels and height of 150 pixels"
   ]
  },
  {
   "source": [
    "## Dataset with no any transformation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create the transformed training dataset\n",
    "\n",
    "# data_transform = transforms.Compose([Rescale(150),\n",
    "#                                      RandomCrop(IMG_SHAPE),\n",
    "#                                      Normalize(),\n",
    "#                                      AddBatchChannel()])\n",
    "\n",
    "data_transform = None\n",
    "\n",
    "trainingDataset = FacialKeypointsDataset(csv_file='data/training_frames_keypoints.csv',\n",
    "                                             root_dir='data/training/',\n",
    "                                             transform=data_transform)\n",
    "\n",
    "# create the transformed testing dataset\n",
    "# data_transform = transforms.Compose([Rescale((IMG_SHAPE, IMG_SHAPE)),\n",
    "#                                      Normalize(),\n",
    "#                                      AddBatchChannel()])\n",
    "\n",
    "data_transform = None\n",
    "\n",
    "\n",
    "testingDataset = FacialKeypointsDataset(csv_file='data/test_frames_keypoints.csv',\n",
    "                                             root_dir='data/test/',\n",
    "                                             transform=data_transform)                                         "
   ]
  },
  {
   "source": [
    "## Dataset with transformation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create the transformed training dataset\n",
    "data_transform = transforms.Compose([Rescale(150),\n",
    "                                     RandomCrop(IMG_SHAPE),\n",
    "                                     Normalize(),\n",
    "                                     AddBatchChannel()])\n",
    "\n",
    "\n",
    "\n",
    "trainingDataset = FacialKeypointsDataset(csv_file='data/training_frames_keypoints.csv',\n",
    "                                             root_dir='data/training/',\n",
    "                                             transform=data_transform)\n",
    "\n",
    "\n",
    "# create the transformed testing dataset\n",
    "data_transform = transforms.Compose([Rescale((IMG_SHAPE, IMG_SHAPE)),\n",
    "                                     Normalize(),\n",
    "                                     AddBatchChannel()])\n",
    "\n",
    "\n",
    "\n",
    "testingDataset = FacialKeypointsDataset(csv_file='data/test_frames_keypoints.csv',\n",
    "                                             root_dir='data/test/',\n",
    "                                             transform=data_transform)                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = trainingDataset[0]\n",
    "\n",
    "print(f\"image shape : {sample['image'].shape} and FKPs shape : {sample['keypoints'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = testingDataset[0]\n",
    "\n",
    "print(f\"image shape : {sample['image'].shape} and FKPs shape : {sample['keypoints'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Train data and Validation data\n",
    "> ### - Train data : 80% train dataset\n",
    "> ### - Validation data : 20% train dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Transform dataset to numpy inputs and targets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformDataset(dataset):\n",
    "    inputs  = []\n",
    "    targets = []\n",
    "\n",
    "    for sample in dataset:\n",
    "        image       = sample['image']\n",
    "        keypoints   = sample['keypoints']\n",
    "\n",
    "        image       = np.array(image)\n",
    "        keypoints   = np.array(keypoints)\n",
    "\n",
    "        inputs.append(image)\n",
    "        targets.append(keypoints)\n",
    "    \n",
    "    inputs  = np.array(inputs)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    return inputs, targets"
   ]
  },
  {
   "source": [
    "## Transformation example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import numpy as np\n",
    "\n",
    "nb = 5\n",
    "array = []\n",
    "fakeData = np.random.rand(nb,  96, 96, 1)\n",
    "\n",
    "for x in fakeData:\n",
    "    print(x.shape)\n",
    "    array.append(x)\n",
    "\n",
    "array = np.array(array)\n",
    "array.shape\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separation de donnees\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT    = 0.20"
   ]
  },
  {
   "source": [
    "## Fake data for testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "inputs.shape : (3000, 96, 96, 1)\ttargets.shape : (3000, 68, 2)\n"
     ]
    }
   ],
   "source": [
    "nb = 3000\n",
    "inputs = np.random.rand(nb,  96, 96, 1)\n",
    "targets = np.random.rand(nb,68,2)\n",
    "\n",
    "print(f\"inputs.shape : {inputs.shape}\\ttargets.shape : {targets.shape}\")"
   ]
  },
  {
   "source": [
    "## Dataset Transformation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = transformDataset(trainingDataset)\n",
    "\n",
    "print(f\"inputs.shape : {inputs.shape}\\ttargets.shape : {targets.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[1].shape"
   ]
  },
  {
   "source": [
    "## Split dataset into training and validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDatas, validationDatas, trainLabels, validationLabels = train_test_split(\n",
    "    inputs,targets, \n",
    "    test_size = VALIDATION_SPLIT, # 0.2\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "# Training datas\n",
    "trainDatas = np.array(trainDatas)\n",
    "trainLabels = np.array(trainLabels)\n",
    "\n",
    "# Validation datas\n",
    "validationDatas = np.array(validationDatas)\n",
    "validationLabels = np.array(validationLabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2400, 96, 96, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "trainDatas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(600, 96, 96, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "validationDatas.shape"
   ]
  },
  {
   "source": [
    "## Testing data transformation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing datas\n",
    "testDatas, testLabels = transformDataset(testingDataset)\n",
    "\n",
    "testDatas = np.array(testDatas)\n",
    "testLabels = np.array(testLabels)\n",
    "\n",
    "print(f\"inputs.shape : {testDatas.shape}\\ttargets.shape : {testLabels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDatas.shape"
   ]
  },
  {
   "source": [
    "## Custom generator for memory optimization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(object):\n",
    "\n",
    "    # class variables\n",
    "    TRAINING = \"training\"\n",
    "    TESTING = \"testing\"\n",
    "    VALIDATION = \"validation\"\n",
    "\n",
    "\n",
    "    def __init__(self, targetLabel=0, targetData='training', batchSize=20):\n",
    "        # targetLabel index must be int\n",
    "        assert isinstance(targetLabel, int)\n",
    "\n",
    "        # targetLata must be str \n",
    "        assert isinstance(targetData, str) and targetData in [self.TRAINING, self.TESTING, self.VALIDATION]\n",
    "\n",
    "        # batchSize index must be int\n",
    "        assert isinstance(batchSize, int)\n",
    "\n",
    "        self.targetLabel = targetLabel\n",
    "        self.targetData = targetData\n",
    "        self.batchSize = batchSize\n",
    "\n",
    "        # counter \n",
    "        self.lastbatch = 0\n",
    "\n",
    "\n",
    "        # compute size\n",
    "        self.size = self.shape()[0]       \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"DataGenerator\\n\\n\"\n",
    "            f\"targetData  : {self.targetData}\\n\"\n",
    "            f\"targetLabel : {self.targetLabel}\\n\"\n",
    "            f\"size        : {self.size}\\n\"\n",
    "            f\"batchSize   : {self.batchSize}\\n\"\n",
    "\n",
    "        )\n",
    "    \n",
    "    def shape(self):\n",
    "        if self.targetData == self.TRAINING:\n",
    "            return trainDatas.shape\n",
    "\n",
    "        elif self.targetData == self.TESTING:\n",
    "            return testDatas.shape\n",
    "\n",
    "        elif self.targetData == self.VALIDATION:\n",
    "            return validationDatas.shape\n",
    "        \n",
    "    def makeBatch(self, datas, targets, selectedIndex):\n",
    "        \"\"\"\n",
    "            Make a batch\n",
    "        \"\"\"\n",
    "\n",
    "        batch = []\n",
    "\n",
    "        for index in range(self.lastbatch, selectedIndex):\n",
    "            # index out of range\n",
    "            if index >= self.size:\n",
    "                break\n",
    "\n",
    "            # sample, image and target FPK\n",
    "            sample = (datas[index], targets[index][self.targetLabel])\n",
    "\n",
    "            batch.append(sample)\n",
    "        \n",
    "        return batch\n",
    "\n",
    "    \n",
    "    def next(self):\n",
    "        # index out of size\n",
    "        if self.lastbatch >= self.size:\n",
    "            self.lastbatch = 0\n",
    "            raise StopIteration()\n",
    "\n",
    "        # batch size\n",
    "        batch = list()\n",
    "\n",
    "        # last index\n",
    "        lastindex = self.lastbatch + self.batchSize\n",
    "\n",
    "        if self.targetData == self.TRAINING:\n",
    "            batch = self.makeBatch(trainDatas, trainLabels, lastindex)\n",
    "\n",
    "        elif self.targetData == self.TESTING:\n",
    "            batch = self.makeBatch(testDatas, testLabels, lastindex)\n",
    "\n",
    "        elif self.targetData == self.VALIDATION:\n",
    "            batch = self.makeBatch(validationDatas, validationLabels, lastindex)\n",
    "\n",
    "        # next batch\n",
    "        self.lastbatch = lastindex\n",
    "\n",
    "        if len(batch) == 0:\n",
    "            self.lastbatch = 0\n",
    "            raise StopIteration()\n",
    "\n",
    "        return batch\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingGenTest = DataGenerator(targetData='training', targetLabel=0, batchSize=20)\n",
    "trainingGenTest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for data in trainingGenTest:\n",
    "    print(f\"Data Size  : {len(data)}\\t- Data Type : {type(data)}\\n\")\n",
    "\n",
    "    sample = data[0]\n",
    "\n",
    "    print(f\"Image size : {sample[0].shape}\\t- Target Size : {sample[1].shape}\\n\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingGenTest = DataGenerator(targetData='testing', targetLabel=0, batchSize=20)\n",
    "testingGenTest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validationGenTest = DataGenerator(targetData='validation', targetLabel=0, batchSize=20)\n",
    "validationGenTest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Defining and Training a Convolutional Neural Network (CNN) to Predict Facial Keypoints"
   ],
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # Use the %tensorflow_version magic if in colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "naimishModel = tf.keras.models.Sequential([\n",
    "    # First layer\n",
    "    tf.keras.layers.Conv2D(32, (4,4), input_shape=(96, 96, 1), activation='elu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "\n",
    "    # second layer\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='elu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "    # Third layer\n",
    "    tf.keras.layers.Conv2D(128, (2,2), activation='elu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "    # Fourth layer\n",
    "    tf.keras.layers.Conv2D(256, (1,1), activation='elu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "\n",
    "    # Flatten\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    # Dense 1\n",
    "    tf.keras.layers.Dense(1000, activation='elu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "    # Dense 2\n",
    "    tf.keras.layers.Dense(1000, activation='elu'),\n",
    "    tf.keras.layers.Dropout(0.6),\n",
    "\n",
    "    # Dense 1\n",
    "    tf.keras.layers.Dense(2, activation='elu'),\n",
    "\n",
    "])"
   ]
  },
  {
   "source": [
    "## Learning Configuration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-08,\n",
    "    amsgrad=False,\n",
    "    name=\"Adam\"\n",
    ")\n",
    "\n",
    "naimishModel.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.metrics.mean_squared_error,\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "source": [
    "## Model Summary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "naimishModel.summary()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 73,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_8 (Conv2D)            (None, 93, 93, 32)        544       \n_________________________________________________________________\nmax_pooling2d_8 (MaxPooling2 (None, 46, 46, 32)        0         \n_________________________________________________________________\ndropout_12 (Dropout)         (None, 46, 46, 32)        0         \n_________________________________________________________________\nconv2d_9 (Conv2D)            (None, 44, 44, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_9 (MaxPooling2 (None, 22, 22, 64)        0         \n_________________________________________________________________\ndropout_13 (Dropout)         (None, 22, 22, 64)        0         \n_________________________________________________________________\nconv2d_10 (Conv2D)           (None, 21, 21, 128)       32896     \n_________________________________________________________________\nmax_pooling2d_10 (MaxPooling (None, 10, 10, 128)       0         \n_________________________________________________________________\ndropout_14 (Dropout)         (None, 10, 10, 128)       0         \n_________________________________________________________________\nconv2d_11 (Conv2D)           (None, 10, 10, 256)       33024     \n_________________________________________________________________\nmax_pooling2d_11 (MaxPooling (None, 5, 5, 256)         0         \n_________________________________________________________________\ndropout_15 (Dropout)         (None, 5, 5, 256)         0         \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 6400)              0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 1000)              6401000   \n_________________________________________________________________\ndropout_16 (Dropout)         (None, 1000)              0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 1000)              1001000   \n_________________________________________________________________\ndropout_17 (Dropout)         (None, 1000)              0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 2)                 2002      \n=================================================================\nTotal params: 7,488,962\nTrainable params: 7,488,962\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "source": [
    "## Test clone keras model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nmodelClone == naimishModel : False\n\nmodelClone is naimishModel : False\n\n\nid(modelClone)   : 2891082457928\n\nid(naimishModel) : 2891792229512\n"
     ]
    }
   ],
   "source": [
    "modelClone = tf.keras.models.clone_model(naimishModel)\n",
    "\n",
    "print(f\"\\nmodelClone == naimishModel : {modelClone == naimishModel}\\n\")\n",
    "print(f\"modelClone is naimishModel : {modelClone is naimishModel}\\n\")\n",
    "print(f\"\\nid(modelClone)   : {id(modelClone)}\\n\\nid(naimishModel) : {id(naimishModel)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelClone.summary()"
   ]
  },
  {
   "source": [
    "# Training\n",
    "\n",
    "> ### - Train data : 80% train dataset\n",
    "> ### - Validation data : 20% train dataset\n",
    "> ### - Batch SIze : 128\n",
    "> ### - Epochs : 300\n",
    "> ### - Early Stopping Callback (ESC) :  stops the training when the number of contiguous epochs without improvement in validation loss are 30 (Patience Level . \n",
    "> ### -  Model Checkpoint Callback (MCC) : stores the weights of the model with the lowest validation loss."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Callback functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE          = 128\n",
    "EPOCHS              = 300\n",
    "PATIENCE            = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataGen = DataGenerator(targetData='validation', targetLabel=0, batchSize=BATCH_SIZE)\n",
    "trainDataGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validationDataGen = DataGenerator(targetData='validation', targetLabel=0, batchSize=BATCH_SIZE)\n",
    "validationDataGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "logDir = os.path.join(cwd, 'logs')\n",
    "\n",
    "if not os.path.isdir(logDir):\n",
    "    os.makedirs(logDir)\n",
    "\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=PATIENCE),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=logDir),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = modelClone.fit(\n",
    "    trainDatas,\n",
    "    trainLabels[:, 0, :],\n",
    "    epochs=EPOCHS,\n",
    "    validation_data = (validationDatas, validationLabels[:, 0, :]),\n",
    "    callbacks=my_callbacks,\n",
    "    verbose=1\n",
    "\n",
    ")"
   ]
  },
  {
   "source": [
    "## Toujours compiler un model apres clonage\n",
    "## You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelClone.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.metrics.mean_squared_error,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 2400 samples, validate on 600 samples\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-eb993b15a48d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidationDatas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidationLabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmy_callbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;31m# Setup work for each epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'metrics'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1084\u001b[1;33m         \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1085\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1086\u001b[0m     \u001b[1;31m# Reset the state of loss metric wrappers.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m     \"\"\"\n\u001b[1;32m--> 199\u001b[1;33m     \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   3069\u001b[0m           \u001b[0massign_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3070\u001b[0m           \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3071\u001b[1;33m         \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3072\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3073\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mget_session\u001b[1;34m(op_input_list)\u001b[0m\n\u001b[0;32m    460\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m       \u001b[0m_initialize_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[1;34m(session)\u001b[0m\n\u001b[0;32m    877\u001b[0m     \u001b[1;31m# marked as initialized.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m     is_initialized = session.run(\n\u001b[1;32m--> 879\u001b[1;33m         [variables_module.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[0;32m    880\u001b[0m     \u001b[0muninitialized_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, op, message)\u001b[0m\n\u001b[0;32m    256\u001b[0m   \"\"\"\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m   \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m     \u001b[1;34m\"\"\"Creates an `InvalidArgumentError`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m     super(InvalidArgumentError, self).__init__(node_def, op, message,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = modelClone.fit(\n",
    "    trainDatas,\n",
    "    trainLabels[:, 0, :],\n",
    "    epochs=1,\n",
    "    validation_data = (validationDatas, validationLabels[:, 0, :]),\n",
    "    callbacks=my_callbacks,\n",
    "    verbose=1\n",
    "\n",
    ")"
   ]
  },
  {
   "source": [
    "# Global and parallel training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFolder(targetPath):\n",
    "    if not os.path.isdir(targetPath):\n",
    "        os.makedirs(targetPath)"
   ]
  },
  {
   "source": [
    "## Save history object Helper"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class HistoryManager(object):\n",
    "    def __init__(self, targetFile, data):\n",
    "        assert isinstance(targetFile, str)\n",
    "\n",
    "        self.filename = f\"{targetFile}.data\"\n",
    "\n",
    "        self.data = data\n",
    "    \n",
    "    def save(self, targetObject=None):\n",
    "        if targetObject == None:\n",
    "            targetObject = self.data\n",
    "\n",
    "        with open(self.filename, 'wb') as handle:\n",
    "            pickle.dump(targetObject, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    def load(self):\n",
    "        with open(self.filename, 'rb') as handle:\n",
    "            data = pickle.load(handle)\n",
    "        \n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\"a\": \"b\"}\n",
    "\n",
    "am = HistoryManager(\"Jonathan\", a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = am.load()\n",
    "b"
   ]
  },
  {
   "source": [
    "## Train model Helper"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabels[:, 0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import RLock\n",
    "\n",
    "\n",
    "def trainModel(model, fkp, my_callbacks, epochs, fkpFolder):\n",
    "\n",
    "    # lock = RLock()\n",
    "\n",
    "    # with lock:\n",
    "    # fit model\n",
    "    history = model.fit(\n",
    "        trainDatas,\n",
    "        trainLabels[:, fkp, :],\n",
    "        epochs=epochs,\n",
    "        validation_data = (validationDatas, validationLabels[:, fkp, :]),\n",
    "        callbacks=my_callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # save history\n",
    "    historyFile = os.path.join(fkpFolder, 'history')\n",
    "\n",
    "    history = HistoryManager(historyFile, history)\n",
    "    history.save()\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Make training Thread"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "# current work directory\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "# current work directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# models folder\n",
    "modelsFolder = os.path.join(cwd, 'models')\n",
    "\n",
    "makeFolder(modelsFolder)\n",
    "\n",
    "# Thread list\n",
    "trainingThreads = []\n",
    "\n",
    "# targetFKPs : index begin by 0\n",
    "targetFKPs = np.array([49, 55, 28, 32, 36, 4, 14]) - 1\n",
    "\n",
    "# iter FKPS\n",
    "for fkp in targetFKPs:\n",
    "    # clone model\n",
    "    modelClone = tf.keras.models.clone_model(naimishModel)\n",
    "\n",
    "    # compile clone model\n",
    "    modelClone.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.metrics.mean_squared_error,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # define callbacks\n",
    "    fkpFolder = os.path.join(modelsFolder, str(f\"FKP{fkp}\"))\n",
    "    makeFolder(fkpFolder)\n",
    "\n",
    "\n",
    "    logDir = os.path.join(fkpFolder, 'logs')\n",
    "    makeFolder(logDir)\n",
    "    \n",
    "    modelFile = f\"{fkpFolder}/\" + 'model.{epoch:02d}-{val_loss:.2f}.h5'\n",
    "\n",
    "    my_callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(patience=PATIENCE),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=modelFile),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir=logDir),\n",
    "    ]\n",
    "\n",
    "    # create training thread\n",
    "    # def trainModel(model, fkp, my_callbacks, epochs, fkpFolder):\n",
    "\n",
    "    # trainThread = Thread(\n",
    "    #     target=trainModel,\n",
    "    #     args=(modelClone, fkp, my_callbacks, 1, fkpFolder),\n",
    "    #     name=f\"TrainingTread#{fkp}\"\n",
    "    # )\n",
    "\n",
    "    # trainModel(modelClone, fkp, my_callbacks, 1, fkpFolder)\n",
    "\n",
    "    history = modelClone.fit(\n",
    "        trainDatas,\n",
    "        trainLabels[:, fkp, :],\n",
    "        epochs=1,\n",
    "        validation_data = (validationDatas, validationLabels[:, fkp, :]),\n",
    "        callbacks=my_callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # add thread to training tread list\n",
    "    # trainingThreads.append(trainThread)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainingThreads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingThreads[0]"
   ]
  },
  {
   "source": [
    "## Start training Threads"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "for trainingTread in trainingThreads:\n",
    "    trainingTread.start()\n",
    "\n",
    "# End of thread\n",
    "for trainingTread in trainingThreads:\n",
    "    trainingTread.join()"
   ]
  },
  {
   "source": [
    "## Result Visualization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Export model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "savedModelPath = os.path.join(os.getcwd(), f\"saved_models/\")\n",
    "\n",
    "if not os.path.exists(savedModelPath):\n",
    "  os.makedirs(savedModelPath)\n",
    "\n",
    "model.trainable = False\n",
    "model.save(savedModelPath)"
   ]
  },
  {
   "source": [
    "## Load saved model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the exact same model, including its weights and the optimizer\n",
    "loadedModel = tf.keras.models.load_model(saved_model_path)\n",
    "\n",
    "# Show the model architecture\n",
    "loadedModel.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit ('tensorflow': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9ec957caba7ae6ccc97a7fb0804bf14cbdb1f70a4904cd45a06dd27fe16a3b19"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}